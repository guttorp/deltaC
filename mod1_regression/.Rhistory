col=rgb(0.5,0.5,0.5,0.5),main="TMIN",lpars=list(col="red"),span=0.2)
scatter.smooth(as.Date(prcp_data$date),prcp_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="PRCP",lpars=list(col="red"),span=0.2)
##Quick plot to check validity.
par(mfrow=c(3,1))
scatter.smooth(as.Date(tmax_data$date),tmax_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMAX",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(tmin_data$date),tmin_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMIN",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(prcp_data$date),prcp_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="PRCP",lpars=list(col="red"),span=0.05)
##Gets the data.
start <- as.Date("1980-09-30")
end <- as.Date("1982-11-30")
tmax_data <- get_ghcn_daily(start,end,type="TMAX",stations=stationIDs,chunk_days=20)[[1]]
tmin_data <- get_ghcn_daily(start,end,type="TMIN",stations=stationIDs,chunk_days=20)[[1]]
prcp_data <- get_ghcn_daily(start,end,type="PRCP",stations=stationIDs,chunk_days=20)[[1]]
##Quick plot to check validity.
par(mfrow=c(3,1))
scatter.smooth(as.Date(tmax_data$date),tmax_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMAX",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(tmin_data$date),tmin_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMIN",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(prcp_data$date),prcp_data$value,data=tmax_data,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="PRCP",lpars=list(col="red"),span=0.05)
##Quick plot to check validity.
par(mfrow=c(3,1))
scatter.smooth(as.Date(tmax_data$date),tmax_data$value,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMAX",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(tmin_data$date),tmin_data$value,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="TMIN",lpars=list(col="red"),span=0.1)
scatter.smooth(as.Date(prcp_data$date),prcp_data$value,pch=20,cex=0.2,
col=rgb(0.5,0.5,0.5,0.5),main="PRCP",lpars=list(col="red"),span=0.05)
head(tmax_data)
tail(tmax_data)
##Gets the data.
start <- as.Date("1905-10-01")
end <- as.Date("2013-09-30")
tmax_data <- get_ghcn_daily(start,end,type="TMAX",stations=stationIDs,chunk_days=20)[[1]]
tmin_data <- get_ghcn_daily(start,end,type="TMIN",stations=stationIDs,chunk_days=20)[[1]]
prcp_data <- get_ghcn_daily(start,end,type="PRCP",stations=stationIDs,chunk_days=20)[[1]]
str(tmax_data)
help(krige.bayes)
library(geoR)
help(krige.bayes)
## Not run:
# generating a simulated data-set
ex.data <- grf(70, cov.pars=c(10, .15), cov.model="matern", kappa=2)
#
# defining the grid of prediction locations:
ex.grid <- as.matrix(expand.grid(seq(0,1,l=21), seq(0,1,l=21)))
#
# computing posterior and predictive distributions
# (warning: the next command can be time demanding)
ex.bayes <- krige.bayes(ex.data, loc=ex.grid,
model = model.control(cov.m="matern", kappa=2),
prior = prior.control(phi.discrete=seq(0, 0.7, l=51),
phi.prior="reciprocal"))
#
# Prior and posterior for the parameter phi
plot(ex.bayes, type="h", tausq.rel = FALSE, col=c("red", "blue"))
#
# Plot histograms with samples from the posterior
par(mfrow=c(3,1))
hist(ex.bayes)
par(mfrow=c(1,1))
# Prior and posterior for the parameter phi
plot(ex.bayes, type="h", tausq.rel = FALSE, col=c("red", "blue"))
#
str(ex.data)
install.packages('spBayes')
help(package="spBayes")
install.packages('rcharts')
library(rcharts)
library(shiny)
runApp()
RunApp()
library(shiny)
runApp()
library(rwbclimate)
install.packages(rwbclimate)
install.packages("rwbclimate")
install.packages("rWBclimate")
install_github("rWBclimate","ropensci")
library(devtools)
install_github("rWBclimate","ropensci")
help(rWBclimate)
library(rWBclimate)
help(rWBclimate)
help(package=rWBclimate)
hist_dat <- get_historical_precip(c("USA","BRA","AUS"),"year")
ggplot(hist_dat,aes(x = year,y = data, group = locator,colour = locator)) + geom_point() + geom_path() + ylab("Mean annual temperature")
hist_dat <- get_historical_precip(c("USA","BRA","AUS","CAN"),"year")
ggplot(hist_dat,aes(x = year,y = data, group = locator,colour = locator)) + geom_point() + geom_path() + ylab("Mean annual temperature")
test <- get_historical_temp("USA")
test <- get_historical_temp("USA","year")
head(test)
ggplot(test,aes(x = year,y = data, group = locator,colour = locator)) + geom_point() + geom_path() + ylab("Mean annual temperature")
test <- get_historical_temp("CAN","year")
ggplot(test,aes(x = year,y = data, group = locator,colour = locator)) + geom_point() + geom_path() + ylab("Mean annual temperature")
?get_historical_temp
data <- get_historical_temp(284,"year")
data
head(data)
colnames(data$year) <- "year"
library(ngramr)
data2 <- ngram("climate",corpus="eng_2012",year_end=2012,smoothing=0)
head(data2)
runApp()
runApp()
runApp()
runApp()
runApp()
goog
goog()
q
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
head(data)
head(data())
runApp()
data
tdata
runApp()
library(shiny)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("~/Desktop/test")
runApp()
runApp()
getwd()
runApp()
library(shiny)
runApp()
options(error = recover)
runApp()
options(error = recover)
runApp()
runApp()
tdata
warnings()
errors()
?options
options(error = NULL)
runApp()
?switch
?tolower
runApp()
runApp()
head(tdata())
str(tdata)
library(shiny)
library(ngramr)
library(rWBclimate)
library(xtable)
test <- ngram("climate",year_end=2012,smoothing=0)
test2 <- get_historical_temp(322,time_scale="year")
head(test)
head(test2)
runApp()
runApp()
runApp()
tdat
tdata
colnames(tdata) <- c("year","data")
head(tdata)
tail(tdata)
runApp()
head(tdata)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?get_historical_temp
runApp()
?ylab
?axis
runApp()
runApp()
lag_data
head(tdata2)
tail(tdata2)
lines(tdata2$year,tdata2$data_lag,lty=2,col='slateblue')
lines(tdata2$year[1:(length(tdata2$year)-lag_factor())],lag_pred,lty=2)
tdata3 <- data.frame(tdata2$data,tdata2$year,data_lag=mean(tdata2$data_lag,na.rm=T))
lag_pred <- predict(model,newdata=tdata3,type='response')
model <- lm(tdata2$data~tdata2$year+tdata2$data_lag)
lag_pred <- predict(model,newdata=tdata3,type='response')
lines(tdata2$year[1:(length(tdata2$year)-lag_factor())],lag_pred,lty=2)
length(lag_pred)
length(tdata2$year[1:(length(tdata2$year)-lag_factor())])
lag_pred <- lag_pred[1:(length(lag_pred)-lag_factor())]
lines(tdata2$year[1:(length(tdata2$year)-lag_factor())],lag_pred,lty=2)
runApp()
?mtext
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(shiny)
runApp()
library(rWBclimate)
tdata <- get_historical_temp(input$ccode,time_scale="year")[,c(1,2)]
tdata <- get_historical_temp("USA",time_scale="year")[,c(1,2)]
tdata
data_ts <- as.ts(tdata$data,frequency=(1/365))
head(data_ts)
str(data_ts)
library(nlme)
?corAR1()
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1(1,form=~1))
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1(1,form=~year))
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1(value=1,form=~year))
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1())
summary(gls)
ols <- lm(data~year,data=tdata)
ols
summary(ols)
summary(gls)
summary(ols)
summary(gls)
summary(ols)
summary(gls)
summary(ols)
acf(ols$residuals)
acf(gls$residuals)
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1(1))
gls <- gls(data~year,data=tdata,method="ML",correlation=corAR1(0.99))
summary(gls)
gls$residuals
acf(gls$residuals)
ar(ts_data,method='ml')
ar(data_ts,method='ml')
gls <- gls(data~year,data=tdata,method="ML",correlation=corARMA(p=5,q=0)
)
gls
acf(gls$residuals)
str(gls)
plot(ACF(gls))
plot(ACF(gls),alpha=0.05)
plot(ACF(gls),alpha=0.01)
plot(ACF(gls),alpha=0.05)
gls <- gls(data~year,data=tdata2,correlation=corARMA(p=1,q=0))
gls <- gls(data~year,data=tdata,correlation=corARMA(p=1,q=0))
summary(model)$coefficients
summary(gls)$coefficients
predict(gls)
plot(predict(gls)~year)
plot(predict(gls)~tdata$year)
str(predict(gls))
residuals(gls,type='response')
plot(residuals(gls,type='response'))
plot(residuals(ols,type='response'))
plot(residuals(gls,type='response'))
plot(residuals(gls,type='standardized'))
plot(residuals(gls,type='pearson'))
plot(residuals(gls,type='normalized'))
residuals(gls,type='normalized')
acf(residuals(gls,type='normalized'))
?rstudent
plot(residuals,ols,type='normalized')
plot(residuals(ols,type='normalized'))
plot(residuals(ols,type='pearson'))
plot(residuals(ols,type='working'))
plot(residuals(ols,type='deviance'))
plot(residuals(ols,type='partial'))
plot(residuals(ols,type='pearson'))
summary(gls)$coefficients
str(gls)
str(summary(gls))
summary(gls)
str(summary(gls))
summary(gls)$phi
summary(gls)$Phi
summary(gls)$tTable
summary(model)$coefficients
summary(ols)$coefficients
summary(gls)$coefficients
summary(gls)$tTable
summary(gls)$tTable
gls(data~year,data=tdata,correlation=corARMA(p=0,q=0))
gls(data~year,data=tdata,correlation=corARMA(p=1,q=0))
gls(data~year,data=tdata,correlation=corARMA(p=0,q=0))
gls(data~year,data=tdata,correlation=corARMA(p=1,q=0))
runApp()
runApp()
runApp()
runApp()
coefs <- summary(gls)$tTable
str(coefs)
?tableOutput
?format
runApp()
runApp()
runApp()
runApp()
runApp()
library(shiny)
runApp()
runApp()
output$downloadData <- downloadHandler({
##Standardizes column names
tdata <- tdata()
colnames(tdata) <- c("year","data")
runApp()
runApp()
library(quantmod)
install.packages("quantmod")
?getMetals
GetMetals
?GetMetals
library(quantmod)
?getMetals
getMetals("gold",auto.assign=F)
getMetals("gold",auto.assign=F,from=Sys.Date() - 300, to=Sys.Date)
getMetals("gold",auto.assign=F,from=Sys.Date() - 300, to=Sys.Date())
plot(getMetals("gold",auto.assign=F,from=Sys.Date() - 300, to=Sys.Date()))
getSymbols('F',auto.assign=F)
shiny::runApp()
##Script to model canopy class as a function of basal area.
##Ian Breckheimer
##11 April 2014
####Sets up the workspace.####
library(rjags)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(reshape2)
library(lme4)# Using the development version of lme4 (on github) which fixes a bug in "predict.merMod()"
library(MASS)
setwd("~/Dropbox/LabGroup/Win 2014/R Code - Independent/ian/can_class")
####Load and prep data####
ab <- read.csv("Data - ABAM tree survival.csv")
ab_high <- read.csv("Data - ABAM tree PARA SUNR SPRY.csv")
## Appends 2013 trees to data frame.
ab <- rbind(ab,ab_high)
## Converts blanks to NA
ab[ab==""] <- NA
## Subsets only live trees
ab <- filter(ab,SURVIVAL=="Alive")
## Removes SRNF and CCNF stands (ABPR stands)
ab <- filter(ab,!(STAND %in% c("SRNF","CCNF","AR19")))
## Removes empty levels of the stand variable and puts stands in alphabetical order.
stands <- as.character(unique(ab$STAND))
stands_order <- stands[order(stands)]
ab$STAND <- factor(ab$STAND,levels=stands_order)
## Creates and ordered factor for canopy class and a binary canopy variable
ab$CANCLASS <- factor(factor(ab$CANCLASS,levels=c("S","I","C","D"),ordered=TRUE))
ab$CanopyYN<- as.numeric(ab$CANCLASS != "S")
## Computes basal area and centered log basal area from dbh
ab$BA <- pi*(ab$DBH/2)^2
ab$log_BA <- log(ab$BA)
ab$log_BA_center <- as.numeric(scale(log(ab$BA),scale=FALSE))
## Creates an indicator of whether the tree is in one of the subplots with seed traps.
ab$TRAP_PLOT <- ab$PLOT %in% c(4,6)
## Converts year to a factor
ab$YEAR_fact <- as.factor(ab$YEAR)
## Centers numeric year.
ab$YEAR_center <- as.numeric(scale(ab$YEAR,scale=FALSE))
## creates a unique variable for each tree.
ab$TREE <- as.factor(paste(ab$STAND,ab$TAG,sep="_"))
## Creates indicator variable for whether the measurement is the latest census for each stand.
abc <- group_by(ab,STAND)
last_census <- summarise(abc,last_census=max(YEAR))
ab$LATEST_CENSUS <- 0
for(i in 1:nrow(last_census)){
last <- which(ab$STAND == last_census[i,1] & ab$YEAR == last_census[i,2])
ab$LATEST_CENSUS[last] <- 1
}
#### Integrates the basal area model with the seed production model.####
## Loads 1000 posterior samples of stand basal area for each trap.
load("jags model - stand.slope.int.meas.ind.check2 - jagsoutput.Rdata")
ba_trap_post <- data.frame(jags.output[[1]][sample(1:1000,1000,replace=FALSE),grep("seeds.ba",colnames(jags.output[[1]]))])
## Brings in seeds data.
seeds_dat <- read.csv("Data - ABAM Seeds.csv")
dat.tmp <- seeds_dat[which(seeds_dat$Basal_area!=0),]
dat.tmp$TrapID <- paste(dat.tmp$Stand,dat.tmp$Quadrat,sep="_")
dat.tmp$Masting <- dat.tmp$Year == 2010 | dat.tmp$Year == 2012
## 3. Neg Binom model, random eff on r, linear link, new masting effect ##
seedmod.negbin3 <- "model {
# Likelihood
# Level 1
# each n = seedtrap
for(i in 1:n){
seeds[i] ~ dnegbin(p[i], r[year[i]] ) # error negative binomially distributed
mu[i] <- slope[year[i]] * basal_area[i] + mast * masting[i] # linear prediction affecting mu
p[i] <- r[year[i]]/(r[year[i]]+mu[i]) # convert mu into p parameter required by JAGS for dnegbin
}
# Level 2 # making both slope and dispersion a random effect of year
for(j in 1:nyears){
slope[j] ~ dlnorm(time.mean, time.tau)    # Precision = 1/variance; large variance = small precision
r[j] ~ dlnorm(r.mean, r.tau)
}
# Level 1 Priors
mast ~ dlnorm(0, 0.0001)
# Level 2 Priors
time.mean ~ dnorm(0, 0.0001) # an uninformative prior???
time.sigma ~ dunif(0,5)
time.tau <- 1/(time.sigma * time.sigma) # an uninformative prior???
r.mean ~ dnorm(0, 0.001) # for a couple of years that don't have much data, this might keep it from wandering so much
r.sigma ~ dunif(0,5)
r.tau <- 1/(r.sigma * r.sigma)
# getting the mean on the scale of the response (since gets loged going through the dlnorm)
time.mean.exp <- exp(time.mean)
r.mean.exp <- exp(r.mean)
}"
# Write model
write(seedmod.negbin3, "seed model - negbin3.txt")
#### Loops through all of the samples from the basal area model and computes seeds per unit basal area####
# Sets up initial data
Basal_area <- as.numeric(ba_trap_post[1,])  # Vector of stand Basal Areas
seeds <- dat.tmp$Seeds # Vector of seed counts
seeds.shift <- seeds + 1 # Vector of seed counts shifted to remove zeros
seeds.scaled <- dat.tmp$Seeds.scaled  # Vector of seeds/m2
stand <- as.numeric(dat.tmp$Stand) # Convert stand names to numeric values
quadrat <- as.numeric(dat.tmp$Quadrat) # Make sure quadrats are numeric
year <- as.numeric(as.factor(dat.tmp$Year)) # Make sure Year is numeric
masting <- rep(0, times=length(year)) # making a dummy variable for masting in years 2010 and 2012
masting[which(dat.tmp$Year==2010 | dat.tmp$Year==2012)] <- 1
nonmasting <- rep(0, times=length(year)) #making the oppositve vector of masting
nonmasting[which(masting==0)] <- 1
n <- length(seeds.scaled)  # Number of seed values
nstands <- max(stand)  # Number of stands
nyears <- length(unique(year))
quadrat <- as.numeric(as.factor(paste(dat.tmp$Stand, dat.tmp$Quadrat)))
nquads <- length(unique(quadrat))
# Assemble data into a list
data.list <- list(seeds= seeds,
n = n,
nyears = nyears,
year = year,
basal_area = Basal_area,
masting= masting)
# Variables to track
trackvars <- c("slope",
"time.mean.exp",
"time.sigma",
"mast",
"r.mean",
"r.sigma")
nsamples <- nrow(ba_trap_post)
post_samples <- data.frame()
autorun_jags <- function(i,trackvars,psrf_thresh=1.1,n_adapt=100,n_update=8000,n_samples=1000){
print(paste("Now performing MCMC run ",i," of ",nsamples))
# Updates basal area with a new posterior estimate
data.list$basal_area <- as.numeric(ba_trap_post[i,])
# Specify initial values for the three chains
init <- expression({list(time.mean = dnorm(1),
time.sigma = dlnorm(1),
r.mean = dnorm(1),
r.sigma = dlnorm(1),
mast = dnorm(1)
)})
inits <- list(eval(init),eval(init),eval(init))
# Runs model
seedmod.negbin3.jagsmodel <- jags.model("seed model - negbin3.txt"
, data = data.list
, n.chains=3
, inits=inits,
, n.adapt=n_adapt,quiet=TRUE)
update(seedmod.negbin3.jagsmodel, n.iter=n_update,quiet=TRUE)
# Checks to make sure the model has converged
output.negbin3 <- coda.samples (model=seedmod.negbin3.jagsmodel, variable.names=trackvars,
n.iter = n_samples, thin=1,quiet=TRUE)
psrf_upperCI <- gelman.diag(output.negbin3)$psrf[,1]
converged <- !any(psrf_upperCI > psrf_thresh)
## Gets a single parameter set from a random chain if the model has converged. Otherwise update again.
if(converged){
print("Convergence check passed, writing output.")
chain <- sample(1:3,1,replace=TRUE)
post_sample <- output.negbin3[[chain]][1,]
names(post_sample) <- dimnames(output.negbin3[[chain]])[[2]]
}
else {
print("Failed convergence check, updating model again.")
update(seedmod.negbin3.jagsmodel, n.iter=n_update)
output.negbin3 <- coda.samples(model=seedmod.negbin3.jagsmodel, variable.names=trackvars,
n.iter = n_samples, thin=1,quiet=TRUE)
chain <- sample(1:3,1,replace=TRUE)
post_sample <- output.negbin3[[chain]][1,]
names(post_sample) <- dimnames(output.negbin3[[chain]])[[2]]
}
return(post_sample)
}
## Runs the analysis in parallel
library(foreach)
library(doMC)
registerDoMC(cores=4)
## Write progress to a log file.
writeLines(c(""), "jags_log.txt")
foreach (i=1:3,.combine=rbind) %dopar% {sink("jags_log.txt",append=TRUE)
autorun_jags(i,trackvars=trackvars,psrf_thresh=1.1,n_adapt=500,n_update=100,n_samples=100)
}
foreach (i=1:3,.combine=rbind) %dopar% {
autorun_jags(i,trackvars=trackvars,psrf_thresh=1.1,n_adapt=500,n_update=100,n_samples=100)
}
autorun_jags(1,n_update=100,n_samples=100)
